{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Vectorization For NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:54:38.126907Z",
     "start_time": "2023-05-24T18:54:37.159034Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:54:42.205007Z",
     "start_time": "2023-05-24T18:54:42.197672Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "#conda install -c anaconda nltk \n",
    "# nltk is not a built-in library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:54:48.210935Z",
     "start_time": "2023-05-24T18:54:44.234033Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:01.078447Z",
     "start_time": "2023-05-24T18:54:53.001192Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kadiryildirim/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kadiryildirim/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/kadiryildirim/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/kadiryildirim/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')       # sentence ve word tokenleştirme işlemleri için gerekli olan dosyaları yükler.\n",
    "nltk.download('stopwords')   # stopword datasetlerini yükler\n",
    "nltk.download('wordnet')     # normalizasyon için gerekli olan dosyaları yükler.\n",
    "nltk.download('omw-1.4')\n",
    "# \"omw-1.4\", bir metinde geçen kelime ve ifadelerin anlamlarını belirlemek için kullanılan bir datasetidir. \n",
    "# Kelimelere/tokenlere uyguladığımız normalization işlemlerinde kelimeleri kökenlerine indirgemek için kelimenin orjinal\n",
    "# hali ile köküne inmiş hali arasındaki anlamsal ilişkiyi kurabilmek için kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:13.535715Z",
     "start_time": "2023-05-24T18:55:13.529950Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's see a sample sentence and spme basic operations\n",
    "sample_text = \"Oh man, this is pretty cool. We will do more such things. 2 ½ % (2) hasn't Ali's books 10.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:22.700903Z",
     "start_time": "2023-05-24T18:55:22.691732Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize # sentence tokenize and word tokenize\n",
    "\n",
    "# sent_tokenize, word_tokenize import ediyoruz. sent_tokenize için sadece bugün bir örnek yapacağız. Bundan sonraki \n",
    "# derslerimizde tokenleştirme işlemlerimizi word_tokenize üzerinden yapacağız.\n",
    "\n",
    "# sent_tokenize daha cok advannced modellerde (translation, sumarrizing) kullanilir. Sent analysis ve\n",
    "# classification icin word_tokenize kullanacagiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:38.464645Z",
     "start_time": "2023-05-24T18:55:38.435927Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh man, this is pretty cool.',\n",
       " 'we will do more such things.',\n",
       " \"2 ½ % (2) hasn't ali's books 10.2\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first lower the text and tokenize. 3 sentences as values of a list\n",
    "sentence_token = sent_tokenize(sample_text.lower())\n",
    "sentence_token\n",
    "\n",
    "# öncelikle string fonksiyonlarından biri olan lower() ile textimizi küçük harfli formata dönüştürüyoruz. Sonrasında cümle\n",
    "# tokenlerine ayırıyoruz. Sentence token işleminde \".\"ları dikkate aldığından her nokta sonrasında cümleler biribirinden\n",
    "# ayrılarak tokenleştirilir.\n",
    "# sentence tokenize çoğunlukla özet çıkarma, translation gibi advence tasklerde kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:42.371295Z",
     "start_time": "2023-05-24T18:55:42.351171Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " '.',\n",
       " '2',\n",
       " '½',\n",
       " '%',\n",
       " '(',\n",
       " '2',\n",
       " ')',\n",
       " 'has',\n",
       " \"n't\",\n",
       " 'ali',\n",
       " \"'s\",\n",
       " 'books',\n",
       " '10.2']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_tokenize and lowerint the text\n",
    "word_token = word_tokenize(sample_text.lower())\n",
    "word_token\n",
    "\n",
    "# öncelikle string fonksiyonlarından biri olan lower() ile textimizi küçük harfli formata dönüştürüyor ve sonrasında kelime\n",
    "# tokenlerine ayırıyoruz. word token işleminde boşlukları dikkate aldığından her boşluk sonrasında kelimeler biribirinden\n",
    "# ayrılarak tokenleştirilir.\n",
    "# Ancak, noktalama işaretleri (üst ayıraç hariç) ve özel karekterler için boşluk şartı aranmaz. Bu işaret veya karakterler ayrı \n",
    "# birer token muamelesi görür.\n",
    "# üst ayraçlar;\n",
    "# 1. olumsuz yardımcı fiiler için olumsuzluk ekleri (hasn't--> \"has\", \"n't\", isn't--> \"is\", \"n't\") gösterildiği\n",
    "#    gibi tokenleştirilir.\n",
    "# 2. iyelik eklerinde (Ali's books--> \"ali\", \"'s\", \"books\") gösterildiği gibi tokenleştirilir.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Punctuation and Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:50.013248Z",
     "start_time": "2023-05-24T18:55:50.004616Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " 'has',\n",
       " 'ali',\n",
       " 'books']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punc = [w for w in word_token if w.isalpha()] # .isalnum() for number and object\n",
    "tokens_without_punc\n",
    "\n",
    "# isalpha() bir tokenin string olup olmadığını kontrol eder. String ise True, değil ise False döndürür.\n",
    "# Yukarıda oluşturduğumuz list comprehension içinde word tokenler teker teker kontrol edilerek string olanlar filtreleniyor.\n",
    "# isalnum() bir tokenin string veya numeric olup olmadığını kontrol eder. String veya numeric ise True, değil ise False döndürür\n",
    "# Eğer numeric ifadelerin de filtrelenmesini istiyorsak isalnum() kullanabiliriz.\n",
    "\n",
    "# Yaptığımız bu filtrelemeyi tokens_without_punc değişkenine atıyoruz.\n",
    "# sayilari ister birakir ister temizleriz. classificationda cok etkileri olmadigi icin tamamen tercih "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:51.640804Z",
     "start_time": "2023-05-24T18:55:51.635800Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:52.286303Z",
     "start_time": "2023-05-24T18:55:52.274719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = stopwords.words(\"english\") # to change the language: for ex. \"turkish\"\n",
    "stop_words\n",
    "\n",
    "# ingilizce için stopwordlarin listesini alıyoruz.\n",
    "# Hangi dile ait stopwordsleri istiyorsak parantez içerisine istediğimiz dili yazıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:52.854887Z",
     "start_time": "2023-05-24T18:55:52.841222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " 'has',\n",
       " 'ali',\n",
       " 'books']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_without_punc\n",
    "\n",
    "# Noktalama işaretleri ve özel karakterlerden temizlenmiş tokenleri şimdi aşağıda olduğu stopwordlerden temizliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:53.485379Z",
     "start_time": "2023-05-24T18:55:53.474160Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'things', 'ali', 'books']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_without_sw = [t for t in tokens_without_punc if t not in stop_words] \n",
    "                                                                          \n",
    "token_without_sw\n",
    "\n",
    "# duygu analizi yapmayacaksanız olumsuz yardımcı fiilleri temizleyebilirsiniz. Ancak yapacaksanız olumsuz yardımcı fiillerin\n",
    "# mutlaka text içerisinde kalması gerekiyor.\n",
    "\n",
    "# Noktalama işaretleri ve özel karakterlerden temizlenmiş tokenleri teker teker çekip stopwordler içerisinde olup olmadığı \n",
    "# kontrol ediliyor. Eğer stopwordler içinde geçmiyorsa textimizde bu token kalmaya devam ediyor. Stopwordler içinde geçiyorsa\n",
    "# ignore ediliyor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization-Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:54.682634Z",
     "start_time": "2023-05-24T18:55:54.658747Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# tokenleri eklerinden kurtarıp köklerine indirgeme işlemine data normalization denir.\n",
    "# Lemmetization ve stemming olarak 2 farklı yöntem vardır.\n",
    "# lemmatization token köklerine inerken orjinal tokenin sözlükte bir karşılığı olup olmadığına yani token köküne inildiğinde\n",
    "# orjinal tokende anlam kaybı olup olmayacağına bakarken stemming böyle bir kontrol ihtiyacı hissetmeden direk köke inebilir.\n",
    "# örnek: \"arabacı\" tokenini köküne indirirken lemmatization \"cı\" ekini atarsam tokende anlam kaybı(arabayı satan kişi, arabaya\n",
    "# dönüşecek) olacağını öngörür ve \"arabacı\" tokenini olduğu gibi bırakır. Ancak stemming direk \"araba\" olarak dönüştürebilir.\n",
    "# Çoğunlukla lemmatization tercih edilir. Ama siz 2'sini de deneyip hangisinde sonuçlarınız iyi ise onunla devam edebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:55:56.777240Z",
     "start_time": "2023-05-24T18:55:55.281441Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'driving'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an example with drive term\n",
    "WordNetLemmatizer().lemmatize(\"driving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T19:17:14.286453Z",
     "start_time": "2023-05-24T19:17:14.264511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "driving\n",
      "driver\n",
      "driver\n",
      "drive\n",
      "drove\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"driving\", \"drivers\", \"driver\", \"drives\", \"drove\"]\n",
    "\n",
    "for i in tokens:\n",
    "    print(WordNetLemmatizer().lemmatize(i))\n",
    "    \n",
    "# drove sürü anlamina da geldigi icin drive'a cevirmedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T19:17:19.317955Z",
     "start_time": "2023-05-24T19:17:19.312533Z"
    }
   },
   "outputs": [],
   "source": [
    "lem = [WordNetLemmatizer().lemmatize(t) for t in token_without_sw]\n",
    "\n",
    "# tüm tokenleri lemmatization yöntemi ile köklerine indirgiyoruz, yani normalization yapiyoruz ve lem değişkenine atıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T19:18:17.072078Z",
     "start_time": "2023-05-24T19:18:17.058129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretty', 'cool', 'thing', 'ali', 'book']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization-Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:01.759978Z",
     "start_time": "2023-05-24T18:56:01.755394Z"
    }
   },
   "outputs": [],
   "source": [
    "# ayni islemi stemming ile deneyelim\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# yukarda da bahsettiğimiz gibi lemmatization token köklerine inerken ineceği kökün sözlükte bir karşılığı olup olmadığına yani \n",
    "# anlam kaybı olup olmayacağına bakarken stemming böyle bir kontrol yapma ihtiyacı hissetmeden direk köke inebilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:02.415028Z",
     "start_time": "2023-05-24T18:56:02.409189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'drive'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PorterStemmer().stem(\"driving\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:03.013140Z",
     "start_time": "2023-05-24T18:56:03.004802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drive\n",
      "driver\n",
      "driver\n",
      "drive\n",
      "drove\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"driving\", \"drivers\", \"driver\", \"drives\", \"drove\"]\n",
    "\n",
    "for i in tokens:\n",
    "    print(PorterStemmer().stem(i))\n",
    "    \n",
    "# ilkinde oldugu gibi driving'i kacirmis v e drivea cevirmis; bu nedenle lemmatization daha cok tercih edilir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:03.645414Z",
     "start_time": "2023-05-24T18:56:03.641057Z"
    }
   },
   "outputs": [],
   "source": [
    "stem = [PorterStemmer().stem(t) for t in token_without_sw]\n",
    "\n",
    "# tüm tokenleri stemming yöntemi ile köklerine indirgiyoruz ve stem değişkenine atıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:04.228591Z",
     "start_time": "2023-05-24T18:56:04.220188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh', 'man', 'pretti', 'cool', 'thing', 'ali', 'book']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem\n",
    "# oldukca anlamindaki pretty'i pretti (guzel) yapti ve anlam kaybina neden oldu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:05.430084Z",
     "start_time": "2023-05-24T18:56:05.408284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh man pretty cool thing ali book'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lem)\n",
    "\n",
    "# son aşamda temizleme işlemi tamamlanan tokenleri join ediyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Function - for classification (NOT for sentiment analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:06.613445Z",
     "start_time": "2023-05-24T18:56:06.604841Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    \n",
    "    #1. Tokenize and lower\n",
    "    text_tokens = word_tokenize(data.lower()) \n",
    "    \n",
    "    #2. Remove Puncs and numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "    \n",
    "    #3. Removing Stopwords\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #4. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #5. joining\n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "# yukarda yaptığımız işlemleri otomatize etmek için temizleme işlemlerini sırasıyla cleaning fonksiyonu içerisine tanımlıyoruz\n",
    "# Eğer sentiment analiz yapılmayacaksa bu fonksiyonu kullanabiliriz. Sentiment analysis not versiyonu\n",
    "# olumsuz yardimci fiillerin kalmasi gerekir; bu nedenle bunu kullanmamaliyiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:07.229761Z",
     "start_time": "2023-05-24T18:56:07.210731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    oh man pretty cool thing ali book\n",
       "dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(sample_text).apply(cleaning) #df[\"text\"].apply(cleaning)\n",
    "\n",
    "# apply fonksiyonu hem df hem de serieslerle birlikte kullanılabildiğinden önce sample_text'i seriese dönüştürüp sonra\n",
    "# apply ile cleaning fonksiyonunu uyguluyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Function - for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:08.477169Z",
     "start_time": "2023-05-24T18:56:08.472632Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_text= \"Oh man, this is pretty cool. We will do more such things. don't aren't are not. no problem\"\n",
    "\n",
    "# eğer bir duygu analizi yapacaksak textimizden olumsuz yardımcı fiileri atmamamız gerektiğini söylemiştik.\n",
    "# örnek için textimizi sample_text değişkenine atıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:09.069371Z",
     "start_time": "2023-05-24T18:56:09.054769Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh man, this is pretty cool. We will do more such things. dont arent are not. no problem'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = sample_text.replace(\"'\",'')\n",
    "s\n",
    "\n",
    "# textimizi stop wordlerden temizlenirken eğer olumsuz yardımcı fiilleri üst ayraçlarından kurtarırsam \"don't\" --> \"dont\"a\n",
    "# \"hasn't\"--> \"hasnt\"a dönüşecek ve stopwordlerden temizleme aşamasında bu tokenler textimizde kalmaya devam edecek. Çünkü,\n",
    "# stopword temizleme aşamasında üst ayraçlı olumsuz yardımcı fiiller temizlenir sadece.\n",
    "\n",
    "# bu durumda Jake's gibi kelimeler Jakes olur ama bunlar keyword olmadigi icin onemli degil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:09.641881Z",
     "start_time": "2023-05-24T18:56:09.625025Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'do',\n",
       " 'more',\n",
       " 'such',\n",
       " 'things',\n",
       " '.',\n",
       " 'dont',\n",
       " 'arent',\n",
       " 'are',\n",
       " 'not',\n",
       " '.',\n",
       " 'no',\n",
       " 'problem']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = word_tokenize(s.lower())\n",
    "word \n",
    "\n",
    "# sample_textimizi önce küçük harflere dönüştürüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:11.338500Z",
     "start_time": "2023-05-24T18:56:11.324356Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words\n",
    "# stop wordlerimizi tekrar görüyoruz. Gördüğünüz gibi olumsuz yardımcı fiiller üst ayraçlı bir şekilde bulunuyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:12.142014Z",
     "start_time": "2023-05-24T18:56:12.133056Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oh',\n",
       " 'man',\n",
       " ',',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " '.',\n",
       " 'things',\n",
       " '.',\n",
       " 'dont',\n",
       " 'arent',\n",
       " '.',\n",
       " 'problem']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_words = [t for t in word if t not in stop_words]\n",
    "cleaning_words\n",
    "\n",
    "# tokenleri teker teker çekip stopwordler içerisinde olup olmadığı kontrol ediliyor. Eğer stopwordler içinde geçmiyorsa \n",
    "# textimizde bu token kalmaya devam ediyor. Stopwordler içinde geçiyorsa ignore ediliyor. \n",
    "\n",
    "# Olumsuz yardımcı fiilleri üst ayraçlarından kurtardığımızdan stopwordlere takılmadan textimizde kalmaya devam ediyorlar.\n",
    "\n",
    "# Ancak gördüğünüz gibi \"not\", \"no\" gibi tektimize olumsuzluk katan tokenler textimizden temizlenmiş. Bunun önüne geçmek için\n",
    "# stopword listesinden remove fonksiyonu ile \"not\" ve \"no\" tokenlerini çıkaracağız. Bu şekilde \"not\" ve \"no\" tokenleri\n",
    "# textimizde kalmaya devam edecek."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:13.009915Z",
     "start_time": "2023-05-24T18:56:12.991892Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in [\"not\", \"no\"]:\n",
    "        stop_words.remove(i)\n",
    "\n",
    "# \"not\" ve \"no\" tokenlerini stop_words listesinden çıkarıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:13.626229Z",
     "start_time": "2023-05-24T18:56:13.620085Z"
    }
   },
   "outputs": [],
   "source": [
    "def cleaning_fsa(data):\n",
    "    \n",
    "    #1. removing upper brackets to keep negative auxiliary verbs in text\n",
    "    text = data.replace(\"'\",'')\n",
    "         \n",
    "    #2. Tokenize and lower\n",
    "    text_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #3. Remove punkt and numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "    \n",
    "    #4. Removing Stopwords     \n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #5. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #6. joining\n",
    "    return \" \".join(text_cleaned)\n",
    "\n",
    "# sentiment analysis icinn bu fonksiyonu kullanabiliriz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:14.308217Z",
     "start_time": "2023-05-24T18:56:14.299716Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['oh man pretty cool thing dont arent not no problem'], dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pd.Series(sample_text).apply(cleaning_fsa))\n",
    "# tum texti gorebilmek icin arraye ceviririz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorization and TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:19.042655Z",
     "start_time": "2023-05-24T18:56:18.846755Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"airline_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:19.932933Z",
     "start_time": "2023-05-24T18:56:19.907951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence   \n",
       "0  570306133677760513           neutral                        1.0000  \\\n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline   \n",
       "0            NaN                        NaN  Virgin America  \\\n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count   \n",
       "0                    NaN     cairdin                 NaN              0  \\\n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord   \n",
       "0                @VirginAmerica What @dhepburn said.         NaN  \\\n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:20.702453Z",
     "start_time": "2023-05-24T18:56:20.651900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14640 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      airline_sentiment                                               text\n",
       "0               neutral                @VirginAmerica What @dhepburn said.\n",
       "1              positive  @VirginAmerica plus you've added commercials t...\n",
       "2               neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3              negative  @VirginAmerica it's really aggressive to blast...\n",
       "4              negative  @VirginAmerica and it's a really big bad thing...\n",
       "...                 ...                                                ...\n",
       "14635          positive  @AmericanAir thank you we got on a different f...\n",
       "14636          negative  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "14637           neutral  @AmericanAir Please bring American Airlines to...\n",
       "14638          negative  @AmericanAir you have my money, you change my ...\n",
       "14639           neutral  @AmericanAir we have 8 ppl so we need 2 know h...\n",
       "\n",
       "[14640 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[['airline_sentiment','text']]\n",
    "df\n",
    "\n",
    "# datamızdan sadece text ve targetimız olan airline_sentiment'i çekiyoruz. Bundan sonra tüm datalarımızda bu şekilde \n",
    "# çalışacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:21.531371Z",
     "start_time": "2023-05-24T18:56:21.519855Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica Really missed a prime opportuni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                @VirginAmerica What @dhepburn said.\n",
       "1          positive  @VirginAmerica plus you've added commercials t...\n",
       "2           neutral  @VirginAmerica I didn't today... Must mean I n...\n",
       "3          negative  @VirginAmerica it's really aggressive to blast...\n",
       "4          negative  @VirginAmerica and it's a really big bad thing...\n",
       "5          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "6          positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "7           neutral  @VirginAmerica Really missed a prime opportuni..."
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.head(8)\n",
    "df\n",
    "\n",
    "# CountVectorization ve TF-IDF Vectorization'ın çalışma mantığını göreceğimizden sadece ilk 8 satır ile çalışacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:22.233072Z",
     "start_time": "2023-05-24T18:56:22.224999Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:22.877640Z",
     "start_time": "2023-05-24T18:56:22.863576Z"
    }
   },
   "outputs": [],
   "source": [
    "df2[\"text\"] = df2[\"text\"].apply(cleaning_fsa)\n",
    "\n",
    "# duygu analizi yapılacağından tektimize cleaning_fsa fonksiyonu ile temizlik işlemi uygulanıyor.\n",
    "# Bu notebookta sadece textimizin nasıl numeric bir forma dönüştürüldüğü gösterilecek. ML algoritmlarına sonraki notebookumuzda\n",
    "# sokacağız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:23.540251Z",
     "start_time": "2023-05-24T18:56:23.525545Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica dhepburn said</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus youve added commercial expe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica didnt today must mean need take ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica seriously would pay flight seat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica yes nearly every time fly vx ear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>neutral</td>\n",
       "      <td>virginamerica really missed prime opportunity ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0           neutral                        virginamerica dhepburn said\n",
       "1          positive  virginamerica plus youve added commercial expe...\n",
       "2           neutral  virginamerica didnt today must mean need take ...\n",
       "3          negative  virginamerica really aggressive blast obnoxiou...\n",
       "4          negative                 virginamerica really big bad thing\n",
       "5          negative  virginamerica seriously would pay flight seat ...\n",
       "6          positive  virginamerica yes nearly every time fly vx ear...\n",
       "7           neutral  virginamerica really missed prime opportunity ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:24.737015Z",
     "start_time": "2023-05-24T18:56:24.730662Z"
    }
   },
   "outputs": [],
   "source": [
    "X = df2[\"text\"]\n",
    "y = df2[\"airline_sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:25.391830Z",
     "start_time": "2023-05-24T18:56:25.384099Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:26.000192Z",
     "start_time": "2023-05-24T18:56:25.954856Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.5, stratify = y, random_state = 42)\n",
    "\n",
    "# datamız 8 satırdan oluştuğundan train ve test datamızı %50 olarak ayırmamız sadece bugüne özel. Yoksa uygulanan bir yöntem \n",
    "# değil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:26.500838Z",
     "start_time": "2023-05-24T18:56:26.494400Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:27.131998Z",
     "start_time": "2023-05-24T18:56:27.117369Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    virginamerica yes nearly every time fly vx ear...\n",
       "0                          virginamerica dhepburn said\n",
       "2    virginamerica didnt today must mean need take ...\n",
       "4                   virginamerica really big bad thing\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:27.691619Z",
     "start_time": "2023-05-24T18:56:27.677372Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "X_test_count = vectorizer.transform(X_test)\n",
    "\n",
    "# data leakagenin önüne geçmek için dönüşümler train datasına fit_transform olarak, test datasına da transform olarak yapılıyor\n",
    "\n",
    "# countvectorizer fit'in yaptığı işlem train setindeki unique bütün tokenleri tespit eder (tüm dönüşümler train setindeki \n",
    "# unique tokenlere göre yapılır)\n",
    "\n",
    "\n",
    "# countvectorizer transformun yaptığı işlem;\n",
    "\n",
    "# Her unique tokenin her yorumda(dokument, row) kaç defa geçtiğini tespit eder (Hem traim hemde test seti için ayrı ayrı).\n",
    "\n",
    "# Dönüşümler train datasındaki unique tokenlere göre yapıldığından train datasında geçen unique tokenler az olursa test datamıza\n",
    "# yapılan dönüşüm esnasında test datasındaki çoğu tokenin ignore olmasına sebebiyet verebiliriz. Bunun önüne geçmek için\n",
    "# train datasının tüm unique tokenleri ihtiva edecek kadar büyük olması gerekir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:28.331707Z",
     "start_time": "2023-05-24T18:56:28.323849Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['another', 'away', 'bad', 'big', 'dhepburn', 'didnt', 'ear',\n",
       "       'every', 'fly', 'go', 'mean', 'must', 'nearly', 'need', 'really',\n",
       "       'said', 'take', 'thing', 'time', 'today', 'trip', 'virginamerica',\n",
       "       'vx', 'worm', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names_out()\n",
    "\n",
    "# get_feature_names_out() ile fit uygulanan train datasındaki unique tokenlerin listesini array olarak çekebiliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:28.973427Z",
     "start_time": "2023-05-24T18:56:28.961254Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x25 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 28 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count\n",
    "\n",
    "# numeric forma dönüştürülmüş olan X_train_count bu şekilde okuyamıyoruz. Bu sebeple toarray() fonksiyonu ile aşağıda olduğu\n",
    "# gibi array'e dönüştüreceğiz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:29.665363Z",
     "start_time": "2023-05-24T18:56:29.650457Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1,\n",
       "        1, 1, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "        0, 0, 0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:30.352575Z",
     "start_time": "2023-05-24T18:56:30.330755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>didnt</th>\n",
       "      <th>ear</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  away  bad  big  dhepburn  didnt  ear  every  fly  go  mean  must   \n",
       "6        0     1    0    0         0      0    1      1    1   1     0     0  \\\n",
       "0        0     0    0    0         1      0    0      0    0   0     0     0   \n",
       "2        1     0    0    0         0      1    0      0    0   0     1     1   \n",
       "4        0     0    1    1         0      0    0      0    0   0     0     0   \n",
       "\n",
       "   nearly  need  really  said  take  thing  time  today  trip  virginamerica   \n",
       "6       1     0       0     0     0      0     1      0     0              1  \\\n",
       "0       0     0       0     1     0      0     0      0     0              1   \n",
       "2       0     1       0     0     1      0     0      1     1              1   \n",
       "4       0     0       1     0     0      1     0      0     0              1   \n",
       "\n",
       "   vx  worm  yes  \n",
       "6   1     1    1  \n",
       "0   0     0    0  \n",
       "2   0     0    0  \n",
       "4   0     0    0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_count = pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names_out(), index=X_train.index)\n",
    "df_train_count\n",
    "\n",
    "# Yukarda elde ettiğimiz arrayin daha okunaklı olabilmesi için df'e dönüştürüp columns isimleri olarak train datasında geçen\n",
    "# unique tokenleri veriyoruz. \n",
    "\n",
    "# Orjinal X_train'indeki yorumların/textlerin numeric dönüşümünlerinin nasıl olduğunu index bazında görebilmek maksadıyla df'e \n",
    "# index olarak X_train.index'i veriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:30.995376Z",
     "start_time": "2023-05-24T18:56:30.986828Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6    virginamerica yes nearly every time fly vx ear...\n",
       "0                          virginamerica dhepburn said\n",
       "2    virginamerica didnt today must mean need take ...\n",
       "4                   virginamerica really big bad thing\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:31.652854Z",
     "start_time": "2023-05-24T18:56:31.640911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx ear worm go away'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6]\n",
    "\n",
    "# orjinal X_train'imizin 6'ıncı indexsinde bulunan yorumun yukardaki df'de nasıl numeric bir forma dönüştüğünü görebiliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:32.247342Z",
     "start_time": "2023-05-24T18:56:32.227380Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>didnt</th>\n",
       "      <th>ear</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  away  bad  big  dhepburn  didnt  ear  every  fly  go  mean  must   \n",
       "3        0     0    0    0         0      0    0      0    0   0     0     0  \\\n",
       "5        0     0    1    0         0      1    0      0    0   0     0     0   \n",
       "1        0     0    0    0         0      0    0      0    0   0     0     0   \n",
       "7        0     0    0    0         0      0    0      0    0   0     0     0   \n",
       "\n",
       "   nearly  need  really  said  take  thing  time  today  trip  virginamerica   \n",
       "3       0     0       1     0     0      0     0      0     0              1  \\\n",
       "5       0     0       1     0     0      1     0      0     0              1   \n",
       "1       0     0       0     0     0      0     0      0     0              1   \n",
       "7       0     0       1     0     0      0     0      0     0              1   \n",
       "\n",
       "   vx  worm  yes  \n",
       "3   0     0    0  \n",
       "5   0     0    0  \n",
       "1   0     0    0  \n",
       "7   0     0    0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_count = pd.DataFrame(X_test_count.toarray(), columns = vectorizer.get_feature_names_out(), index = X_test.index)\n",
    "df_test_count\n",
    "\n",
    "# test datasına yapılan dönüşüm train datasındaki unique tokenlere göre yapıldığından çoğu tokenin ignore edildiğini \n",
    "# görebiliyoruz. Bunun sebebi train datamızın çok küçük olması. Bu tür sorunların önüne geçebilmek için train datamızın\n",
    "# tüm olası tokenleri ihtiva edecek kadar mümkün olduğunca büyük olması gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:32.867584Z",
     "start_time": "2023-05-24T18:56:32.854500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    virginamerica really aggressive blast obnoxiou...\n",
       "5    virginamerica seriously would pay flight seat ...\n",
       "1    virginamerica plus youve added commercial expe...\n",
       "7    virginamerica really missed prime opportunity ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:33.543823Z",
     "start_time": "2023-05-24T18:56:33.536566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica really aggressive blast obnoxious entertainment guest face amp little recourse'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[3]\n",
    "# testteki transform train datasindaki unique tokenlara gore yapildi; train datamiz da cok kucuktu; bu nedenle\n",
    "# testteki bircok unique wordu yakalayamadi, yok cunku trainde. bu nednele cok kotu bir tahmin etti.\n",
    "\n",
    "# bu nedenle NLP modellerinde train datasinin tum tokenlari icerecek kadar buyuk olmasi cok onemli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:34.153080Z",
     "start_time": "2023-05-24T18:56:34.140951Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'virginamerica': 21,\n",
       " 'yes': 24,\n",
       " 'nearly': 12,\n",
       " 'every': 7,\n",
       " 'time': 18,\n",
       " 'fly': 8,\n",
       " 'vx': 22,\n",
       " 'ear': 6,\n",
       " 'worm': 23,\n",
       " 'go': 9,\n",
       " 'away': 1,\n",
       " 'dhepburn': 4,\n",
       " 'said': 15,\n",
       " 'didnt': 5,\n",
       " 'today': 19,\n",
       " 'must': 11,\n",
       " 'mean': 10,\n",
       " 'need': 13,\n",
       " 'take': 16,\n",
       " 'another': 0,\n",
       " 'trip': 20,\n",
       " 'really': 14,\n",
       " 'big': 3,\n",
       " 'bad': 2,\n",
       " 'thing': 17}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_\n",
    "\n",
    "# fit işlemi uygulanan train datasındaki unique tokenlerinden toplam sayısı."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn TD-IDF\n",
    "https://towardsdatascience.com/how-sklearns-tf-idf-is-different-from-the-standard-tf-idf-275fa582e73d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:35.863329Z",
     "start_time": "2023-05-24T18:56:35.860079Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:36.484144Z",
     "start_time": "2023-05-24T18:56:36.441607Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)\n",
    "\n",
    "# data leakagenin önüne geçmek için dönüşümler train datasına fit_transform olarak, test datasına da transform olarak yapılıyor\n",
    "\n",
    "# TF-IDF fit'in yaptığı işlem train setindeki unique bütün tokenleri tespit eder (tüm dönüşümler train setindeki unique\n",
    "# tokenlere göre yapılır)\n",
    "\n",
    "\n",
    "# TF-IDF transformun yaptığı işlem;\n",
    "# a = Her unique tokenin her satırda/yorumda kaçar defa geçtiğini tespit eder (Hem train hemde test seti için ayrı ayrı).\n",
    "# b = Her satırın/yorumun(dokument, row) kaç tokenden oluştuğunu tespit eder (Hem train hemde test seti için ayrı ayrı).\n",
    "# c = Her unique tokenin kaç satırda/yorumda (document) geçtiğini tespit eder (Hem train hemde test seti için ayrı ayrı).\n",
    "# d = datanın toplam kaç satırdan/yorumdan oluştuğunu tespit eder (Hem train hemde test seti için ayrı ayrı)\n",
    "# bu değerleri aşağıdaki formulde yerlerine koyup hesaplamasını yapar.\n",
    "# TF = a/b, DF = c/d, \n",
    "# IDF = log10(d/c+1)--> log10(0) olması durumunda sonsuz/tanımsız olacağından sklearn +1 ilave eder. \n",
    "# TF-IDF = TF x IDF\n",
    "\n",
    "# TF-IDF (train datası için) = train datasındaki a, b, c,d bilgilerine göre hesaplanır.\n",
    "# TF-IDF (test datası için)  = test  datasındaki a, b, c,d bilgilerine göre hesaplanır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:37.056846Z",
     "start_time": "2023-05-24T18:56:37.044940Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['another', 'away', 'bad', 'big', 'dhepburn', 'didnt', 'ear',\n",
       "       'every', 'fly', 'go', 'mean', 'must', 'nearly', 'need', 'really',\n",
       "       'said', 'take', 'thing', 'time', 'today', 'trip', 'virginamerica',\n",
       "       'vx', 'worm', 'yes'], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# get_feature_names_out() ile fit uygulanan train datasındaki unique tokenlerin listesini array olarak çekebiliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:37.585624Z",
     "start_time": "2023-05-24T18:56:37.570778Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.31200802, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.31200802, 0.31200802, 0.31200802, 0.31200802,\n",
       "        0.        , 0.        , 0.31200802, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.31200802, 0.        ,\n",
       "        0.        , 0.16281873, 0.31200802, 0.31200802, 0.31200802],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.66338461,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.66338461, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.34618161, 0.        , 0.        , 0.        ],\n",
       "       [0.34768534, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34768534, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.34768534, 0.34768534, 0.        , 0.34768534, 0.        ,\n",
       "        0.        , 0.34768534, 0.        , 0.        , 0.34768534,\n",
       "        0.34768534, 0.18143663, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.48380259, 0.48380259, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.48380259,\n",
       "        0.        , 0.        , 0.48380259, 0.        , 0.        ,\n",
       "        0.        , 0.25246826, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tf_idf.toarray()\n",
    "\n",
    "# array'e dönüştürüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:38.119538Z",
     "start_time": "2023-05-24T18:56:38.092592Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>didnt</th>\n",
       "      <th>ear</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.162819</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "      <td>0.312008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.663385</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.346182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.347685</td>\n",
       "      <td>0.181437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.252468</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    another      away       bad       big  dhepburn     didnt       ear   \n",
       "6  0.000000  0.312008  0.000000  0.000000  0.000000  0.000000  0.312008  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.663385  0.000000  0.000000   \n",
       "2  0.347685  0.000000  0.000000  0.000000  0.000000  0.347685  0.000000   \n",
       "4  0.000000  0.000000  0.483803  0.483803  0.000000  0.000000  0.000000   \n",
       "\n",
       "      every       fly        go      mean      must    nearly      need   \n",
       "6  0.312008  0.312008  0.312008  0.000000  0.000000  0.312008  0.000000  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.347685  0.347685  0.000000  0.347685   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "     really      said      take     thing      time     today      trip   \n",
       "6  0.000000  0.000000  0.000000  0.000000  0.312008  0.000000  0.000000  \\\n",
       "0  0.000000  0.663385  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.347685  0.000000  0.000000  0.347685  0.347685   \n",
       "4  0.483803  0.000000  0.000000  0.483803  0.000000  0.000000  0.000000   \n",
       "\n",
       "   virginamerica        vx      worm       yes  \n",
       "6       0.162819  0.312008  0.312008  0.312008  \n",
       "0       0.346182  0.000000  0.000000  0.000000  \n",
       "2       0.181437  0.000000  0.000000  0.000000  \n",
       "4       0.252468  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_tfidf = pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names_out(), \n",
    "                              index= X_train.index)\n",
    "df_train_tfidf\n",
    "\n",
    "# Yukarda elde ettiğimiz arrayin daha okunaklı olabilmesi için df'e dönüştürüp columns isimleri olarak train datasında geçen\n",
    "# unique tokenleri veriyoruz. \n",
    "\n",
    "# Orjinal X_train'indeki yorumların numeric dönüşümünlerinin nasıl olduğunu index bazında görebilmek maksadıyla df'e index \n",
    "# olarak X_train.index'i veriyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:38.618339Z",
     "start_time": "2023-05-24T18:56:38.608156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica yes nearly every time fly vx ear worm go away'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[6]\n",
    "\n",
    "# orjinal X_train'imizin 6'ıncı indexsinde bulunan yorumun yukardaki df'de nasıl numeric bir forma dönüştüğünü görebiliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:39.157315Z",
     "start_time": "2023-05-24T18:56:39.140687Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "another          0.347685\n",
       "didnt            0.347685\n",
       "trip             0.347685\n",
       "today            0.347685\n",
       "mean             0.347685\n",
       "must             0.347685\n",
       "take             0.347685\n",
       "need             0.347685\n",
       "virginamerica    0.181437\n",
       "said             0.000000\n",
       "worm             0.000000\n",
       "vx               0.000000\n",
       "time             0.000000\n",
       "thing            0.000000\n",
       "nearly           0.000000\n",
       "really           0.000000\n",
       "away             0.000000\n",
       "go               0.000000\n",
       "fly              0.000000\n",
       "every            0.000000\n",
       "ear              0.000000\n",
       "dhepburn         0.000000\n",
       "big              0.000000\n",
       "bad              0.000000\n",
       "yes              0.000000\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_tfidf.loc[2].sort_values(ascending=False)\n",
    "\n",
    "# TF-IDF textimizde sıklıkla geçen tokenlere ait TF-IDF katsayılarını minimize ederek bu tekenleri önemsizleştirir.\n",
    "\n",
    "# TF-IDF ile dönüştürülmüş bir text ML veya DL modeline verildiğinde model öncelikle yüksek katsayı atanmış tokenlere \n",
    "# yoğunlaşır. \n",
    "\n",
    "# virginamerica her yorumda geçtiği için diğer tokenlere göre daha küçük bir katsayı almış."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:39.667619Z",
     "start_time": "2023-05-24T18:56:39.637981Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>away</th>\n",
       "      <th>bad</th>\n",
       "      <th>big</th>\n",
       "      <th>dhepburn</th>\n",
       "      <th>didnt</th>\n",
       "      <th>ear</th>\n",
       "      <th>every</th>\n",
       "      <th>fly</th>\n",
       "      <th>go</th>\n",
       "      <th>mean</th>\n",
       "      <th>must</th>\n",
       "      <th>nearly</th>\n",
       "      <th>need</th>\n",
       "      <th>really</th>\n",
       "      <th>said</th>\n",
       "      <th>take</th>\n",
       "      <th>thing</th>\n",
       "      <th>time</th>\n",
       "      <th>today</th>\n",
       "      <th>trip</th>\n",
       "      <th>virginamerica</th>\n",
       "      <th>vx</th>\n",
       "      <th>worm</th>\n",
       "      <th>yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.483803</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.252468</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.886548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462637</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  away       bad  big  dhepburn     didnt  ear  every  fly   go   \n",
       "3      0.0   0.0  0.000000  0.0       0.0  0.000000  0.0    0.0  0.0  0.0  \\\n",
       "5      0.0   0.0  0.483803  0.0       0.0  0.483803  0.0    0.0  0.0  0.0   \n",
       "1      0.0   0.0  0.000000  0.0       0.0  0.000000  0.0    0.0  0.0  0.0   \n",
       "7      0.0   0.0  0.000000  0.0       0.0  0.000000  0.0    0.0  0.0  0.0   \n",
       "\n",
       "   mean  must  nearly  need    really  said  take     thing  time  today   \n",
       "3   0.0   0.0     0.0   0.0  0.886548   0.0   0.0  0.000000   0.0    0.0  \\\n",
       "5   0.0   0.0     0.0   0.0  0.483803   0.0   0.0  0.483803   0.0    0.0   \n",
       "1   0.0   0.0     0.0   0.0  0.000000   0.0   0.0  0.000000   0.0    0.0   \n",
       "7   0.0   0.0     0.0   0.0  0.886548   0.0   0.0  0.000000   0.0    0.0   \n",
       "\n",
       "   trip  virginamerica   vx  worm  yes  \n",
       "3   0.0       0.462637  0.0   0.0  0.0  \n",
       "5   0.0       0.252468  0.0   0.0  0.0  \n",
       "1   0.0       1.000000  0.0   0.0  0.0  \n",
       "7   0.0       0.462637  0.0   0.0  0.0  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_tfidf=pd.DataFrame(X_test_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names_out(), index = X_test.index)\n",
    "df_test_tfidf\n",
    "\n",
    "# test datasına yapılan dönüşüm train datasındaki unique tokenlere göre yapıldığından çoğu tokenin ignore edildiğini \n",
    "# görebiliyoruz. Bunun sebebi train datamızın çok küçük olması. Bu tür sorunların önüne geçebilmek için train datamızın\n",
    "# tüm olası tokenleri ihtiva edecek kadar mümkün olduğunca büyük olması gerekiyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:40.187073Z",
     "start_time": "2023-05-24T18:56:40.176525Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica really aggressive blast obnoxious entertainment guest face amp little recourse'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[3]\n",
    "\n",
    "# df'deki dönüşümüne bakıldığında çoğu tokenin iğnore edildiğini görebiliyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:42.202736Z",
     "start_time": "2023-05-24T18:56:42.193426Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.5228787452803376"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(3/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:43.179622Z",
     "start_time": "2023-05-24T18:56:43.167302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5228787452803376"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(100/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:43.741692Z",
     "start_time": "2023-05-24T18:56:43.724406Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.522878745280337"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(10000000/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:44.301360Z",
     "start_time": "2023-05-24T18:56:44.294414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06020599913279624"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(2)*1/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:44.816660Z",
     "start_time": "2023-05-24T18:56:44.809376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-24T18:56:45.356875Z",
     "start_time": "2023-05-24T18:56:45.348817Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
